{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eYlCkVLTxho",
        "outputId": "2e282abc-4b13-45ea-ff8e-cfc60997dd28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… System Ready: Initializing Research-Grade Effort Engine...\n",
            "â³ Loading Data...\n",
            "ðŸ› ï¸ Generating 'Sprint2Vec' Context Features...\n",
            "ðŸ§  Generating Text Embeddings...\n",
            "   âœ… TF-IDF Fitted successfully. Vocab size: 100\n",
            "\n",
            "ðŸš€ Training Quantile Models (Pinball Loss)...\n",
            "   - Training Lower Bound (10th Percentile)...\n",
            "   - Training Median (50th Percentile)...\n",
            "   - Training Upper Bound (90th Percentile)...\n",
            "âœ… Models Trained.\n",
            "\n",
            "ðŸ“Š Evaluating Performance...\n",
            "   Mean Absolute Error: 0.34 Points\n",
            "   Interval Coverage:   96.1%\n",
            "\n",
            "ðŸ’¾ Saving Artifacts for Main.py...\n",
            "âœ… SUCCESS! Download these 4 files and upload them to 'backend/models/':\n",
            "   1. effort_model_lower.json\n",
            "   2. effort_model_median.json\n",
            "   3. effort_model_upper.json\n",
            "   4. effort_artifacts.pkl\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ðŸ“š AGILE EFFORT PREDICTION: RESEARCH-GRADE QUANTILE REGRESSION (FIXED)\n",
        "# =============================================================================\n",
        "# FIXES:\n",
        "# 1. Solved \"idf vector is not fitted\" by forcing fit_transform before saving.\n",
        "# 2. Implements Quantile Regression (Lower/Median/Upper) for confidence intervals.\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score, confusion_matrix\n",
        "import joblib\n",
        "import warnings\n",
        "\n",
        "# Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style(\"whitegrid\")\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"âœ… System Ready: Initializing Research-Grade Effort Engine...\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. DATA LOADING & CLEANING\n",
        "# ==========================================\n",
        "print(\"â³ Loading Data...\")\n",
        "# Ensure this CSV is uploaded to your notebook environment\n",
        "df_raw = pd.read_csv('agile_event_stream_dataset_6k.csv')\n",
        "\n",
        "# Filter: We only want standard Fibonacci points (Agile Standard)\n",
        "valid_points = [1, 2, 3, 5, 8, 13]\n",
        "df = df_raw[df_raw['Story_Point'].isin(valid_points)].copy()\n",
        "\n",
        "# ==========================================\n",
        "# 2. SPRINT2VEC (CONTEXT EMBEDDINGS)\n",
        "# ==========================================\n",
        "print(\"ðŸ› ï¸ Generating 'Sprint2Vec' Context Features...\")\n",
        "\n",
        "df['Creation_Date'] = pd.to_datetime(df['Creation_Date_Change'])\n",
        "df = df.sort_values('Creation_Date')\n",
        "\n",
        "# A. Momentum (Team Velocity in last 14 days)\n",
        "df['team_velocity_14d'] = df.rolling('14D', on='Creation_Date')['Story_Point'].sum()\n",
        "\n",
        "# B. Sprint Load (Active tickets at creation time)\n",
        "df['sprint_load_7d'] = df.rolling('7D', on='Creation_Date')['Issue_ID'].count()\n",
        "\n",
        "# C. Sprint Pressure (Points / Time)\n",
        "df['days_remaining'] = 14 - df['days_since_sprint_start']\n",
        "df['pressure_index'] = df['Story_Point'] / df['days_remaining'].clip(lower=0.5)\n",
        "\n",
        "# ==========================================\n",
        "# 3. TEXT EMBEDDINGS (THE FIX)\n",
        "# ==========================================\n",
        "print(\"ðŸ§  Generating Text Embeddings...\")\n",
        "\n",
        "df['full_text'] = df['Title'].fillna('') + \" \" + df['Description'].fillna('')\n",
        "\n",
        "# Initialize\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=100,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "# ðŸš¨ CRITICAL FIX: fit_transform MUST run here\n",
        "# This calculates the vocabulary and IDF weights needed for the backend\n",
        "text_emb = tfidf.fit_transform(df['full_text']).toarray()\n",
        "\n",
        "# Verify it is fitted\n",
        "if not hasattr(tfidf, 'idf_'):\n",
        "    raise ValueError(\"âŒ CRITICAL ERROR: TF-IDF did not fit! Check your data.\")\n",
        "else:\n",
        "    print(f\"   âœ… TF-IDF Fitted successfully. Vocab size: {len(tfidf.vocabulary_)}\")\n",
        "\n",
        "# Add to DataFrame\n",
        "text_cols = [f\"txt_{i}\" for i in range(text_emb.shape[1])]\n",
        "df_text = pd.DataFrame(text_emb, columns=text_cols, index=df.index)\n",
        "df = pd.concat([df, df_text], axis=1)\n",
        "\n",
        "# Categorical Encodings\n",
        "le_type = LabelEncoder()\n",
        "df['Type_Code'] = le_type.fit_transform(df['Type'].astype(str))\n",
        "\n",
        "# ==========================================\n",
        "# 4. PREPARE DATASETS\n",
        "# ==========================================\n",
        "features = ['sprint_load_7d', 'team_velocity_14d', 'pressure_index',\n",
        "            'total_links', 'Type_Code'] + text_cols\n",
        "target = 'Story_Point'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ==========================================\n",
        "# 5. TRAIN QUANTILE REGRESSION (XGBOOST)\n",
        "# ==========================================\n",
        "print(\"\\nðŸš€ Training Quantile Models (Pinball Loss)...\")\n",
        "\n",
        "def train_quantile(alpha):\n",
        "    model = xgb.XGBRegressor(\n",
        "        objective='reg:quantileerror',\n",
        "        quantile_alpha=alpha,\n",
        "        n_estimators=600,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=5,\n",
        "        subsample=0.8,\n",
        "        n_jobs=-1,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "print(\"   - Training Lower Bound (10th Percentile)...\")\n",
        "model_lower = train_quantile(0.10)\n",
        "\n",
        "print(\"   - Training Median (50th Percentile)...\")\n",
        "model_median = train_quantile(0.50)\n",
        "\n",
        "print(\"   - Training Upper Bound (90th Percentile)...\")\n",
        "model_upper = train_quantile(0.90)\n",
        "\n",
        "print(\"âœ… Models Trained.\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. EVALUATION\n",
        "# ==========================================\n",
        "print(\"\\nðŸ“Š Evaluating Performance...\")\n",
        "\n",
        "pred_raw = model_median.predict(X_test)\n",
        "lower_bound = model_lower.predict(X_test)\n",
        "upper_bound = model_upper.predict(X_test)\n",
        "\n",
        "# Practical accuracy check (Â±1 Fibonacci level)\n",
        "fibs = np.array([1, 2, 3, 5, 8, 13])\n",
        "def snap_to_fib(val): return fibs[(np.abs(fibs - val)).argmin()]\n",
        "pred_fib = [snap_to_fib(p) for p in pred_raw]\n",
        "mae = mean_absolute_error(y_test, pred_raw)\n",
        "\n",
        "print(f\"   Mean Absolute Error: {mae:.2f} Points\")\n",
        "print(f\"   Interval Coverage:   {np.mean((y_test >= lower_bound) & (y_test <= upper_bound))*100:.1f}%\")\n",
        "\n",
        "# ==========================================\n",
        "# 7. SAVE ARTIFACTS (CRITICAL STEP)\n",
        "# ==========================================\n",
        "print(\"\\nðŸ’¾ Saving Artifacts for Main.py...\")\n",
        "\n",
        "# A. Save the 3 XGBoost Models\n",
        "model_lower.save_model(\"effort_model_lower.json\")\n",
        "model_median.save_model(\"effort_model_median.json\")\n",
        "model_upper.save_model(\"effort_model_upper.json\")\n",
        "\n",
        "# B. Save the Preprocessors (The \"Brain\")\n",
        "# Note: We save 'tfidf' which we CONFIRMED is fitted in Step 3\n",
        "artifacts = {\n",
        "    'tfidf': tfidf,\n",
        "    'le_type': le_type\n",
        "}\n",
        "joblib.dump(artifacts, \"effort_artifacts.pkl\")\n",
        "\n",
        "print(\"âœ… SUCCESS! Download these 4 files and upload them to 'backend/models/':\")\n",
        "print(\"   1. effort_model_lower.json\")\n",
        "print(\"   2. effort_model_median.json\")\n",
        "print(\"   3. effort_model_upper.json\")\n",
        "print(\"   4. effort_artifacts.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# 1. Load your dataset\n",
        "csv_path = 'agile_event_stream_dataset_6k.csv'\n",
        "\n",
        "if not os.path.exists(csv_path):\n",
        "    print(f\"âŒ Error: Could not find '{csv_path}'. Please ensure it is in this folder.\")\n",
        "    exit()\n",
        "\n",
        "print(\"â³ Loading dataset...\")\n",
        "df_raw = pd.read_csv(csv_path)\n",
        "\n",
        "# Filter for standard points to match your training logic\n",
        "valid_points = [1, 2, 3, 5, 8, 13]\n",
        "df = df_raw[df_raw['Story_Point'].isin(valid_points)].copy()\n",
        "\n",
        "# Prepare Text (Title + Description)\n",
        "df['full_text'] = df['Title'].fillna('') + \" \" + df['Description'].fillna('')\n",
        "\n",
        "# 2. FIT TF-IDF (The Critical Fix)\n",
        "print(\"ðŸ§  Training TF-IDF Vectorizer...\")\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=100,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "# This line calculates the IDF weights. Without this, the model crashes.\n",
        "tfidf.fit(df['full_text'])\n",
        "\n",
        "# Check if it worked\n",
        "if hasattr(tfidf, 'idf_'):\n",
        "    print(\"   âœ… TF-IDF fitted successfully (Vocabulary & Weights present).\")\n",
        "else:\n",
        "    print(\"   âŒ TF-IDF failed to fit.\")\n",
        "    exit()\n",
        "\n",
        "# 3. FIT LabelEncoder\n",
        "print(\"ðŸ·ï¸ Training Label Encoder...\")\n",
        "le_type = LabelEncoder()\n",
        "le_type.fit(df['Type'].astype(str))\n",
        "\n",
        "# 4. Save the Artifacts\n",
        "output_file = \"effort_artifacts.pkl\"\n",
        "artifacts = {\n",
        "    'tfidf': tfidf,\n",
        "    'le_type': le_type\n",
        "}\n",
        "\n",
        "joblib.dump(artifacts, output_file)\n",
        "print(f\"\\nâœ… SUCCESS! Generated '{output_file}'.\")\n",
        "print(\"ðŸ‘‰ Now move this file to your backend/models/ folder and restart the server.\")"
      ],
      "metadata": {
        "id": "WItIgjJWUneV",
        "outputId": "26921cda-4905-401e-b9fd-df1cf67681aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â³ Loading dataset...\n",
            "ðŸ§  Training TF-IDF Vectorizer...\n",
            "   âœ… TF-IDF fitted successfully (Vocabulary & Weights present).\n",
            "ðŸ·ï¸ Training Label Encoder...\n",
            "\n",
            "âœ… SUCCESS! Generated 'effort_artifacts.pkl'.\n",
            "ðŸ‘‰ Now move this file to your backend/models/ folder and restart the server.\n"
          ]
        }
      ]
    }
  ]
}